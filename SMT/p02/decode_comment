#!/usr/bin/env python
import optparse
import sys
import models
from collections import namedtuple

optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="data/input", help="File containing sentences to translate (default=data/input)")
optparser.add_option("-t", "--translation-model", dest="tm", default="data/tm", help="File containing translation model (default=data/tm)")
optparser.add_option("-l", "--language-model", dest="lm", default="data/lm", help="File containing ARPA-format language model (default=data/lm)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k", "--translations-per-phrase", dest="k", default=1, type="int", help="Limit on number of translations to consider per phrase (default=1)")
optparser.add_option("-s", "--stack-size", dest="s", default=1, type="int", help="Maximum stack size (default=1)")
optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,  help="Verbose mode (default=off)")
opts = optparser.parse_args()[0]

#
# read translation model from the file data/tm, set pruning prameter k=1 (default)
tm = models.TM(opts.tm, opts.k)
# read language model from the file data/lm
lm = models.LM(opts.lm)
# both models are defined in "models.py"
#
# read an input file line by line in a list called "french", each line = french sentence
# each element in this list is a tuple of words of the input line
french = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]
#
# tm should translate unknown words as-is with probability 1
# for each word in the list french
for word in set(sum(french,())):
  # if word is unknown
  if (word,) not in tm:
    # save a word in a translation model with a logarithmic probability 0 (e.t. with probability 1)
    tm[(word,)] = [models.phrase(word, 0.0)]
#
# output in a standart out
sys.stderr.write("Decoding %s...\n" % (opts.input,))
#
# start decoding algorithm
# for each sentence in french
for f in french:
  # The following code implements a monotone decoding
  # algorithm (one that doesn't permute the target phrases).
  # Hence all hypotheses in stacks[i] represent translations of 
  # the first i words of the input sentence. You should generalize
  # this so that they can represent translations of *any* i words.
  #
  # define new tuple names "hypothesis". Each hypothesis consist of logarithmic porbability, ????? lm_state, predecessor and phrase
  hypothesis = namedtuple("hypothesis", "logprob, lm_state, predecessor, phrase")
  # define start (empty) hypothesis
  initial_hypothesis = hypothesis(0.0, lm.begin(), None, None) 
  #
  # 
  # define a set of stacks (implemented as a list of dictionarys, 
  # each element of the dictionary is a translation "hypothesis")
  # there are so many stacks a words in sentence f + 1
  stacks = [{} for _ in f] + [{}]
  #
  # first stack contains only one hypothesis : the empty hypothesis
  stacks[0][lm.begin()] = initial_hypothesis
  #
  # for each stack
  for i, stack in enumerate(stacks[:-1]):
    #
    # sort translation hypothesis from the stack in decreasing order of logarithmic probabilitys
    # and consider first s elements (pruning with the stack size s, default s=1)
    for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:opts.s]:
      #
      # for each survived hypothesis
      #
      # for the rest foreign words in f 
      for j in xrange(i+1,len(f)+1):
	#
	# if tm knows a new built phrase f[i:j] 
        if f[i:j] in tm:
	  # for all translation options
          for phrase in tm[f[i:j]]:
	    # recalculate probability of the hypothesis extanded with a new phrase
            logprob = h.logprob + phrase.logprob
            # save current state of the translation ()
            lm_state = h.lm_state
            # for each english word in the phrase
            for word in phrase.english.split():
	      # calculate a probability of each english word
              (lm_state, word_logprob) = lm.score(lm_state, word)
              logprob += word_logprob
            # add a probability of the last word, if we reached the end of the foreign sentence  
            logprob += lm.end(lm_state) if j == len(f) else 0.0
            # create new hypothesis
            new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
            # expanded hypothesis is places in corresponding stack
            if lm_state not in stacks[j] or stacks[j][lm_state].logprob < logprob: # second case is recombination
              stacks[j][lm_state] = new_hypothesis 
    # for the current hypothesis h in stack i calculate a translation with a maximal probability              
  winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
  #
  # define function "extract_english", which sets together an english translation for a given hypothesis
  def extract_english(h): 
    return "" if h.predecessor is None else "%s%s " % (extract_english(h.predecessor), h.phrase.english)
  #
  # print an englisch translation for hypothesis with a maximal probability 
  print extract_english(winner)
  #
  # if we want to see a verbose information of the translation process
  if opts.verbose:
    # define a function "extract_tm_logprob", which returns a logarithmic probability of a given hypothesis
    def extract_tm_logprob(h):
      return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
    #
    # calculate logarithmic probability of the best translation-hypothesis
    tm_logprob = extract_tm_logprob(winner)
    #  
    # print on a standart out information about ????? 
    sys.stderr.write("LM = %f, TM = %f, Total = %f\n" % 
      (winner.logprob - tm_logprob, tm_logprob, winner.logprob))
